{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5d83ac-fe4d-41f8-b33a-ef72fbf8cbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## TODO Will want access to input vendor set used by norme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74295ec-1e6a-448b-87e3-dc551a911f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "Customer_Name = \"tmobile\"\n",
    "Env = \"prod\"\n",
    "api_token = dbutils.secrets.get(scope=\"suplari-secrets\", key=f\"{Customer_Name}-{Env}\")\n",
    "\n",
    "suplari_env = f\"{Customer_Name}/initial40_databricks_test\"\n",
    "\n",
    "# Generate a unique X-Suplari-Request-ID with Databricks identifier\n",
    "request_id = f\"Databricks-GetNormalizationReport-{datetime.now().strftime('%Y%m%d')}-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "url = f'https://{Customer_Name}.suplari.com/v1/normalization/report'\n",
    "headers = {\n",
    "      'Authorization': f'Bearer {api_token}',\n",
    "      'Content-Type': 'application/json',\n",
    "      'X-Suplari-Environment': suplari_env,\n",
    "      'X-Suplari-Request-ID': request_id\n",
    "    }\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "data = StringIO(response.text)\n",
    "# df = spark.createDataFrame(pd.read_csv(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0e8383-bd60-4753-bddb-e7eebc21588d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17c312b-f01e-441b-8b42-235476b52d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27837f8a-8837-417c-b01a-22abf9873eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example Vendor Normzalization Plan\n",
    "Unit tests for each component\n",
    "Integration tests for the full pipeline\n",
    "Validation tests to ensure data quality\n",
    "\n",
    "Implementation Timeline\n",
    "\n",
    "1. Environment setup and configuration - 1 day\n",
    "2. Data access implementations - 2 days\n",
    "3. Normalization logic implementation - 3 days\n",
    "4. Pipeline orchestration - 2 days\n",
    "5. Testing and validation - 3 days\n",
    "6. Documentation and deployment - 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "785c6163-4c8f-4a00-a38c-b14f0e95cdab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VendorNormalization\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "603c0826-855e-492f-b713-c360ffbca62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(input_path, reference_table_path):\n",
    "    # Load input data from S3\n",
    "    input_vendors = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(input_path)\n",
    "    \n",
    "    # Load reference table\n",
    "    reference_table = spark.read.table(reference_table_path)\n",
    "    \n",
    "    return input_vendors, reference_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4fa5037-d93f-46d9-b4e6-42bcf2947cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_vendors(input_vendors, reference_table):\n",
    "    # Join with reference table\n",
    "    normalized_vendors = input_vendors.join(\n",
    "        reference_table,\n",
    "        input_vendors.original_vendor == reference_table.original_vendor,\n",
    "        \"left_outer\"\n",
    "    )\n",
    "    \n",
    "    # Apply normalization logic - use existing normalization if available\n",
    "    normalized_vendors = normalized_vendors.withColumn(\n",
    "        \"normalized_vendor\",\n",
    "        when(col(\"is_normalized\") == True, col(\"normalized_vendor\"))\n",
    "        .otherwise(col(\"original_vendor\"))\n",
    "    )\n",
    "    \n",
    "    # Select required columns for output\n",
    "    result = normalized_vendors.select(\n",
    "        \"original_vendor\",\n",
    "        \"normalized_vendor\",\n",
    "        \"website\"\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04c11eb-2c2b-4ee6-8753-5b2a84a04232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(normalized_data, output_path):\n",
    "    # Write results to output location\n",
    "    normalized_data.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417238c4-631f-458d-a0fe-081a80117168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_normalization_pipeline(input_path, reference_table_path, output_path):\n",
    "    try:\n",
    "        # Extract\n",
    "        input_vendors, reference_table = extract_data(input_path, reference_table_path)\n",
    "        \n",
    "        # Transform\n",
    "        normalized_vendors = normalize_vendors(input_vendors, reference_table)\n",
    "        \n",
    "        # Load\n",
    "        load_data(normalized_vendors, output_path)\n",
    "        \n",
    "        return True, \"Normalization completed successfully\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error in normalization process: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac03c94-488d-4266-82c4-d523d1910f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Example usage with error handling\n",
    "def execute_with_retry(func, max_retries=3, *args, **kwargs):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {retries+1} failed: {str(e)}\")\n",
    "            retries += 1\n",
    "            if retries == max_retries:\n",
    "                logger.error(f\"Max retries reached. Operation failed.\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868afb78-64e0-4322-bb33-5d25c36c2dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Normalization Testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
